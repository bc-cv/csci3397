{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuUAgQFQlsJU"
   },
   "source": [
    "# [CSCI 3397/PSYC 3317] Lab 4: Image Registration\n",
    "\n",
    "**Posted:** Monday, February 9, 2026\n",
    "\n",
    "**Due:** Monday, February 16, 2026\n",
    "\n",
    "__Total Points__: 8 pts\n",
    "\n",
    "__Submission__: please rename the .ipynb file as __\\<your_username\\>_lab4.ipynb__ before you submit it to canvas. Example: weidf_lab4.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IooAewe2_Hk1"
   },
   "outputs": [],
   "source": [
    "# download image\n",
    "! wget https://bc-cv.github.io/csci3397/public/dip_registration/exemplar-001-cycle-01.ome.tiff -O exemplar-001-cycle-01.ome.tiff\n",
    "! wget https://bc-cv.github.io/csci3397/public/dip_registration/exemplar-001-cycle-03.ome.tiff -O exemplar-001-cycle-03.ome.tiff\n",
    "! wget https://bc-cv.github.io/csci3397/public/dip_registration/view_1.jpg -O view_1.jpg\n",
    "! wget https://bc-cv.github.io/csci3397/public/dip_registration/view_2.jpg -O view_2.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlcW1Sw0_Rb9"
   },
   "outputs": [],
   "source": [
    "# auxilary functions for visualization\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import Image as Image_dsp\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "\n",
    "\n",
    "def plotImgPair(img1, img2, cm = 'gray', title=['','']):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 20))\n",
    "    for i,a in enumerate(ax):\n",
    "        a.set_axis_off()\n",
    "        if title[i] != '':\n",
    "            a.set_title(title[i])\n",
    "    ax[0].imshow(img1, cmap=cm)\n",
    "    ax[1].imshow(img2, cmap=cm)\n",
    "\n",
    "def autoContrast(I, thres=[1,99]):\n",
    "    # compute percentile: remove too big or too small values\n",
    "    I_low, I_high = np.percentile(I.reshape(-1), thres)\n",
    "    # thresholding\n",
    "    I[I > I_high] = I_high\n",
    "    I[I < I_low] = I_low\n",
    "    # scale to 0-1\n",
    "    I = (I.astype(float)- I_low)/ (I_high-I_low)\n",
    "    # convert it to uint8\n",
    "    I = (I * 255).astype(np.uint8)\n",
    "    return I\n",
    "\n",
    "def dispGif(img1, img2, filename='tmp.gif'):\n",
    "    im1 =  Image.fromarray(img1)\n",
    "    im2 =  Image.fromarray(img2)\n",
    "    im1.save(filename, format='GIF',\n",
    "                   append_images=[im2],\n",
    "                   save_all=True,\n",
    "                   duration=300, loop=0)\n",
    "    return Image_dsp(open(filename,'rb').read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weMMc2WZlsJc"
   },
   "source": [
    "# <b>1. Image Transformation (Lec. 8)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee14rhT1x3uC"
   },
   "source": [
    "## 1.1 Visualize image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUSwununx3uC"
   },
   "outputs": [],
   "source": [
    "from imageio import volread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# \"imAdjust()\" defined in auxilary functions\n",
    "img1 = autoContrast(volread('exemplar-001-cycle-01.ome.tiff')[0,:512,:512])\n",
    "img2 = autoContrast(volread('exemplar-001-cycle-03.ome.tiff')[0,:512,:512])\n",
    "\n",
    "# \"plotImgPair()\" defined in auxilary functions\n",
    "# hard to tell the difference\n",
    "plotImgPair(img1, img2, title=['img1', 'img2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhhpg4U-x3uD"
   },
   "outputs": [],
   "source": [
    "# RGB visualization\n",
    "sz = img1.shape\n",
    "out = np.zeros([sz[0], sz[1], 3], np.uint8)\n",
    "out[:,:, 0] = img1\n",
    "out[:,:, 1] = img2\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(out);plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrgUwRmlx3uD"
   },
   "outputs": [],
   "source": [
    "# GIF visualization\n",
    "dispGif(img1, img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSzKfJN-x3t8"
   },
   "source": [
    "## 1.2 Point cloud representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AU_Iqq-Bx3t9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "im_size = [3,3]\n",
    "I = 100 * np.arange(np.prod(im_size)).reshape(im_size)\n",
    "\n",
    "print('Matrix representation:\\n', I)\n",
    "\n",
    "[x,y] = np.meshgrid(np.arange(im_size[0]), np.arange(im_size[1]))\n",
    "\n",
    "print('------------')\n",
    "print('Point cloud representation (x,y,I):')\n",
    "print(np.hstack([x.reshape(-1,1),y.reshape(-1,1),I.reshape(-1,1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouLM1WvNx3t_"
   },
   "source": [
    "## 1.3 Transformation\n",
    "The benefit of forward warping is that it's straightforward to visualize what happens to the pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLXKDz0ox3t_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "T_trans = np.array([[1,0,2],[0,1,3]])\n",
    "T_scale = np.array([[2,0,0],[0,1.5,0]])\n",
    "theta= 45/180*np.pi\n",
    "T_rotate = np.array([[np.cos(theta),np.sin(theta),0],\\\n",
    "                     [-np.sin(theta),np.cos(theta),0]])\n",
    "\n",
    "\n",
    "T=[T_trans, T_scale, T_rotate]\n",
    "names=['translation','scale','rotation']\n",
    "\n",
    "pos = np.vstack([x.reshape(-1), y.reshape(-1), np.ones(y.size)])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i,t in enumerate(T):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.plot(x.reshape(-1), y.reshape(-1),'b.')\n",
    "    # numpy matrix multiplication\n",
    "    pos2 = np.matmul(t, pos)\n",
    "    plt.plot(pos2[0], pos2[1],'rx')\n",
    "    plt.xlim([-2,6])\n",
    "    plt.ylim([-2,6])\n",
    "    plt.title(names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbNXWOlXx3uB"
   },
   "source": [
    "# <b>2. Transformation estimation (Lec. 9) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7ijTZXQlsMC"
   },
   "source": [
    "## 2.1 Template matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34cv7N9nlsMI"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "crop_start = [256,256]\n",
    "crop_size = [64,64]\n",
    "img2_template = img2[crop_start[0] : crop_start[0] + crop_size[0],\\\n",
    "                    crop_start[1] : crop_start[1] + crop_size[1]]\n",
    "\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(15, 20))\n",
    "ax1.imshow(img2, cmap='gray')\n",
    "rect = plt.Rectangle((crop_start[0], crop_start[1]), crop_size[0], crop_size[1], edgecolor='r', facecolor='none')\n",
    "ax1.add_patch(rect)\n",
    "ax2.imshow(img2_template, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6zu2H4Ex3uE"
   },
   "source": [
    "### (a) Cross-correlation\n",
    "(same as convolution, except no flipping kernel)\n",
    "\n",
    "<img src=\"https://docs.opencv.org/2.4/_images/math/93f1747a86a3c5095a0e6a187442c6e2a0ae0968.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6WWM0hwx3uF"
   },
   "outputs": [],
   "source": [
    "# the maximum location is at the bottom-right border\n",
    "# the result won't be bad if we increase the crop size\n",
    "heatmap_cc = cv2.matchTemplate(img1, img2_template, cv2.TM_CCORR)\n",
    "plt.imshow(heatmap_cc, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv9mHTXSx3uF"
   },
   "source": [
    "### (b) Normalized Cross-correlation\n",
    "For each patch, divide the correlations result by the norm of both patches.\n",
    "\n",
    "<img src=\"https://docs.opencv.org/2.4/_images/math/6a72ad9ae17c4dad88e33ed16308fc1cfba549b8.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmy9ITuMx3uF"
   },
   "outputs": [],
   "source": [
    "# now the maximum\n",
    "heatmap_ncc = cv2.matchTemplate(img1, img2_template, cv2.TM_CCORR_NORMED)\n",
    "plt.imshow(heatmap_ncc, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgCr8iS9x3uG"
   },
   "source": [
    "### (c) Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63bel0H8x3uG"
   },
   "outputs": [],
   "source": [
    "ij = np.unravel_index(np.argmax(heatmap_ncc), heatmap_ncc.shape)\n",
    "match_x, match_y = ij[::-1]\n",
    "\n",
    "print('shift by: (%d,%d)' % (match_x-crop_start[0], match_y-crop_start[1]))\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(15, 20))\n",
    "ax1.imshow(img1, cmap='gray')\n",
    "rect = plt.Rectangle((match_x, match_y), crop_size[0], crop_size[1], edgecolor='r', facecolor='none')\n",
    "ax1.add_patch(rect)\n",
    "\n",
    "ax2.imshow(img2_template, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBl_aHJ2x3uG"
   },
   "source": [
    "### (d) Warp img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3yXT7dmx3uG"
   },
   "outputs": [],
   "source": [
    "A = np.ones([2,3])\n",
    "A[1,0] = 0\n",
    "A[0,1] = 0\n",
    "A[0,2] = -crop_start[0] + match_x\n",
    "A[1,2] = -crop_start[1] + match_y\n",
    "\n",
    "img2_trans = cv2.warpAffine(img2, A, img1.shape)\n",
    "\n",
    "dispGif(img1, img2_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kouE6Au0x3uH"
   },
   "outputs": [],
   "source": [
    "# Note that the rgb visualization is not as sensitive to show the error\n",
    "out = np.zeros([img1.shape[0], img1.shape[1], 3], np.uint8)\n",
    "out[:,:, 0] = img1\n",
    "out[:,:, 1] = img2_trans\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(out);plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEG82aeFx3uH"
   },
   "source": [
    "## 2.2 3-Step Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btfOdx6Gx3uH"
   },
   "source": [
    "### (a) Step 1. Get the key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTMOqGsnx3uH"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def get_orb_features(img):\n",
    "    '''\n",
    "    Compute ORB features using cv2 library functions (hint: you will need cv2.ORB_create() and some related functions).  Use default parameters when computing the keypoints.\n",
    "    Input:\n",
    "      img: cv2 image\n",
    "    Returns:\n",
    "      keypoints: a list of cv2 keypoints\n",
    "      descriptors: a list of ORB descriptors\n",
    "    '''\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "\n",
    "kp_1, desc_1 = get_orb_features(img1)\n",
    "kp_2, desc_2 = get_orb_features(img2)\n",
    "\n",
    "kp_img1 = cv2.drawKeypoints(img1, kp_1, None, color=(0,255,0), flags=0)\n",
    "kp_img2 = cv2.drawKeypoints(img2, kp_2, None, color=(0,255,0), flags=0)\n",
    "\n",
    "plotImgPair(kp_img1, kp_img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5RideFax3uI"
   },
   "source": [
    "### (b) Step 2. Find matches among key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxzJtjesx3uI"
   },
   "outputs": [],
   "source": [
    "def match_keypoints(desc_1, desc_2):\n",
    "    '''\n",
    "    Compute matches between feature descriptors of two images using ratio test.\n",
    "    You may use cv2 library functions.(hint: you may need to use cv2.DescriptorMatcher_create or cv2.BFMatcher and some related functions)\n",
    "    Input:\n",
    "      desc_1, desc_2: list of feature descriptors\n",
    "    Return:\n",
    "      matches: list of feature matches\n",
    "    '''\n",
    "    matcher = cv2.DescriptorMatcher_create(\"BruteForce-Hamming\")\n",
    "    matches = matcher.match(desc_1, desc_2)\n",
    "    return matches\n",
    "\n",
    "# display the top-20 matches\n",
    "# note that some matches are quite off (e.g., big tree branches in the left image)\n",
    "matches = match_keypoints(desc_1, desc_2)\n",
    "match_plot = cv2.drawMatches(img1, kp_1, img2, kp_2, matches[:20], None, flags=2)\n",
    "print(\"orb feature matches\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(match_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VOv0QWbx3uI"
   },
   "source": [
    "### (c) Step 3. Estimate transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN397Pqwx3uI"
   },
   "outputs": [],
   "source": [
    "pts_1 = np.vstack([kp_1[m.queryIdx].pt for m in matches])\n",
    "pts_2 = np.vstack([kp_2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "T_aff, status = cv2.estimateAffine2D(pts_2, pts_1, cv2.RANSAC)\n",
    "\n",
    "print(T_aff)\n",
    "print('Number of input pairs: %d\\nNumber of matched pairs: %d' % (len(status), status.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HXqjd92Deey"
   },
   "source": [
    "## 2.3 [Demo] Panoroma stitching\n",
    "\n",
    "Organize the code in section 2.2 into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM8uJq2TDe3A"
   },
   "outputs": [],
   "source": [
    "def panoramic_stitching(img1, img2, do_display=True):\n",
    "  '''\n",
    "    Given a pair of overlapping images, generate a panoramic image.\n",
    "    Hint: use the functions that you've written in the previous parts.\n",
    "    Input:\n",
    "      img1, img2: cv2 images\n",
    "    Return:\n",
    "      final_img: cv2 image of panorama\n",
    "  '''\n",
    "  # 1.detect keypoints and extract orb feature descriptors\n",
    "  kp_1, desc_1 = get_orb_features(img1)\n",
    "  kp_2, desc_2 = get_orb_features(img2)\n",
    "\n",
    "  # 2.match features between two images\n",
    "  matches = match_keypoints(desc_1, desc_2)\n",
    "  pts_2 = np.vstack([kp_2[m.trainIdx].pt for m in matches])\n",
    "  pts_1 = np.vstack([kp_1[m.queryIdx].pt for m in matches])\n",
    "\n",
    "\n",
    "  # 3.compute homography matrix H transforming points from pts_2 to pts_1. Note the order here (not pts_1 to pts_2)!\n",
    "  T_aff, status = cv2.estimateAffine2D(pts_2, pts_1, cv2.RANSAC)\n",
    "\n",
    "  # 4. apply perspective wrap to stitch images together\n",
    "  # use img1 as the canvas\n",
    "  # rough approximation of the output image size\n",
    "  output_size = (img2.shape[1] + img1.shape[1], img2.shape[0] + img1.shape[0])\n",
    "\n",
    "  # warp img2 with H\n",
    "  final_img = cv2.warpAffine(img2, T_aff, output_size)\n",
    "\n",
    "  # copy img1 to the final image\n",
    "  # pixels in img1 remain in the same position in the original position\n",
    "  final_img[0:img1.shape[0], 0:img1.shape[1]] = img1\n",
    "\n",
    "  # crop the empty region\n",
    "  col_sum = final_img.max(axis=0)\n",
    "  row_sum = final_img.max(axis=1)\n",
    "  last_col = np.where(col_sum!=0)[0][-1]\n",
    "  last_row = np.where(row_sum!=0)[0][-1]\n",
    "\n",
    "  final_img = final_img[:last_row, :last_col]\n",
    "\n",
    "  if do_display:\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(final_img)\n",
    "    plt.show()\n",
    "\n",
    "  return final_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpsT1LY_GIUB"
   },
   "outputs": [],
   "source": [
    "# Align the two images above\n",
    "result = panoramic_stitching(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXOOyil6FJrg"
   },
   "outputs": [],
   "source": [
    "# Let's look at some natural images that are not a simple shift\n",
    "plotImgPair(imread('view_1.jpg'), imread('view_2.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyOECl_YGF7W"
   },
   "outputs": [],
   "source": [
    "result = panoramic_stitching(imread('view_1.jpg'), imread('view_2.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLZxOOH-lsU3"
   },
   "source": [
    "# [8 pts] Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdYrYkkTx3uJ"
   },
   "source": [
    "## (1) [2 pts] Visualize an affine transformation\n",
    "\n",
    "In sec. 1.2, we showed how to plot the forward warping with translation, scale, and rotation. Here, write down an arbitrary affine transformation matrix and transform the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u0BRMUXx3uJ"
   },
   "outputs": [],
   "source": [
    "# make image (dots) to test on\n",
    "im_size = [3,3]\n",
    "I = 100 * np.arange(np.prod(im_size)).reshape(im_size)\n",
    "[x,y] = np.meshgrid(np.arange(im_size[0]), np.arange(im_size[1]))\n",
    "pos = np.vstack([x.reshape(-1), y.reshape(-1), np.ones(y.size)])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x.reshape(-1), y.reshape(-1),'b.')\n",
    "\n",
    "### Your code starts here\n",
    "# define transformation\n",
    "T_affine = ???\n",
    "# transform pos\n",
    "pos2 = ???\n",
    "### Your code ends here\n",
    "\n",
    "plt.plot(pos2[0], pos2[1],'rx')\n",
    "plt.title('Arbitrary affine transformation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOQy82NIx3uJ"
   },
   "source": [
    "## (2) [2 pts] Bilinear interpolation\n",
    "\n",
    "Lec. 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63cbQV2Xx3uJ"
   },
   "outputs": [],
   "source": [
    "def bilinearInterp(mat, pos):\n",
    "    # mat: [[I0,I1],[I2,I3]], 2x2 numpy matrix with values at four corners\n",
    "    # pos: (\\alpha, \\beta), numpy array for the position\n",
    "    val = 0\n",
    "\n",
    "    ### Your code starts here\n",
    "\n",
    "\n",
    "    ### Your code ends here\n",
    "    return val\n",
    "\n",
    "## test case\n",
    "\n",
    "test_mat = np.array([[50,100],[150,200]])\n",
    "test_pos = [0.7, 0.3]\n",
    "\n",
    "print(bilinearInterp(test_mat, test_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMhvSKMdx3uK"
   },
   "source": [
    "## (3) [4 pts] Visualize Sec. 2.2 results\n",
    "\n",
    "- Run through sec. 2.2 to get affine transformation of img2\n",
    "- Visualize both the RGB and Gif results here.\n",
    "\n",
    "The example here is simple and the simple translation works similarly to affine transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HU8PIB2Ox3uK"
   },
   "outputs": [],
   "source": [
    "### Your code starts\n",
    "# [1 pt]\n",
    "img2_trans_aff = ???\n",
    "\n",
    "# 1. [2 pts] RGB visualization of results\n",
    "out = ???\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(out);plt.axis('off')\n",
    "plt.show();\n",
    "\n",
    "# 2. [1 pt] gif visualization of results\n",
    "???\n",
    "### Your code ends"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1_d49_U8Zrz_Ac-QWgXFBzr4CJ4S8fKe1",
     "timestamp": 1614174979410
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "csci3397",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
