{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/spmallick/learnopencv/blob/master/PyTorch-Segmentation-torchvision/intro-seg.ipynb","timestamp":1635126597366}]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"KMdbwOEbeQ0s"},"source":["# [CSCI 3397/PSYC 3317] Lab 10: Image Segmentation and Generation\n","\n","**Posted:** Friday, April 19, 2024\n","\n","**Due:** Friday, April 26, 2024\n","\n","__Total Points__: 9 pts"]},{"cell_type":"markdown","source":["# <b>1. Image segmentation</b>"],"metadata":{"id":"AUPSCb5p6pn_"}},{"cell_type":"markdown","source":["## 1.1. FCN: Fully convolutional network\n","\n","**Acknowledgement**: `\n","Satya Mallick: https://github.com/spmallick/learnopencv`"],"metadata":{"id":"sV5uA0w06vNT"}},{"cell_type":"markdown","source":["### (a) Data"],"metadata":{"id":"OJdLRnE29MjJ"}},{"cell_type":"markdown","source":[" #### Download and visualization"],"metadata":{"id":"-hyudwtf9zuA"}},{"cell_type":"code","source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","!wget -nv \"https://www.learnopencv.com/wp-content/uploads/2021/01/person-segmentation.jpeg\" -O person.png\n","img = Image.open('./person.png')\n","height=480\n","img = img.resize((int(float(img.size[0])*height/float(img.size[1])),height))\n","\n","plt.rcParams[\"figure.figsize\"] = (5,5)\n","plt.imshow(img); plt.show()"],"metadata":{"id":"XXnK1o8e9NBa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Pre-process\n","\n","- Convert it to Tensor - all the values in the image becomes between `[0, 1]` from `[0, 255]`\n","- Normalize it with the Imagenet specific values `mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]`\n","\n","And lastly, we unsqueeze the image so that it becomes `[1 x C x H x W]` from `[C x H x W]` <br/>\n","We need a batch dimension while passing it to the models."],"metadata":{"id":"OGoCXj5G9OLI"}},{"cell_type":"code","source":["# Apply the transformations needed\n","import torchvision.transforms as T\n","trf = T.Compose([T.ToTensor(),\n","                 T.Normalize(mean = [0.485, 0.456, 0.406],\n","                             std = [0.229, 0.224, 0.225])])\n","img_pt = trf(img).unsqueeze(0)"],"metadata":{"id":"Gm3BX4i99RTj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (b) Model examination\n"],"metadata":{"id":"mRME_s5Y9TFG"}},{"cell_type":"code","source":["import torch\n","from torchvision import models\n","fcn = models.segmentation.fcn_resnet50(pretrained=True)"],"metadata":{"id":"GuhG62Vk9UP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fcn.modules"],"metadata":{"id":"JXxyLCIP9Vro"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (c) Inference"],"metadata":{"id":"-WHkM4sz9ahE"}},{"cell_type":"code","source":["# turn on the evaluation mode\n","fcn.eval()\n","\n","# only the deep learning part\n","with torch.no_grad():\n","  backbone_output = fcn.backbone(img_pt)\n","print('Network output shape:', backbone_output['out'].shape)"],"metadata":{"id":"cy4Ak6b49ZMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the base model forward() has interpolation\n","# https://github.com/pytorch/vision/blob/9ae833af31a20e3a5113bfca30dc34ac708000d8/torchvision/models/segmentation/_utils.py#L27\n","with torch.no_grad():\n","  out = fcn(img_pt)['out']\n","print('Final output shape:', out.shape)"],"metadata":{"id":"Ymf8CHBc_62I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Visualization\n","The model was trained on `21` classes and thus our output have `21` channels!\n","We take a max index for each pixel position, which represents the class"],"metadata":{"id":"8vkvNfzd9ckk"}},{"cell_type":"code","source":["out_pred = out[0].argmax(0)\n","\n","# create a color pallette, selecting a color for each class\n","palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n","colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n","colors = (colors % 255).numpy().astype(\"uint8\")\n","\n","# plot the semantic segmentation predictions of 21 classes in each color\n","r = Image.fromarray(out_pred.byte().cpu().numpy()).resize(img.size)\n","r.putpalette(colors)\n","\n","import matplotlib.pyplot as plt\n","from skimage.color import label2rgb\n","import numpy as np\n","\n","plt.rcParams[\"figure.figsize\"] = (30,10)\n","plt.subplot(131)\n","plt.imshow(img);plt.axis('equal');plt.axis('off')\n","plt.subplot(132)\n","plt.imshow(r);plt.axis('equal');plt.axis('off')\n","plt.subplot(133)\n","plt.imshow(label2rgb(out_pred.cpu().numpy(), image=np.array(img)));plt.axis('equal');plt.axis('off')\n","\n","plt.show()"],"metadata":{"id":"V9qyMF3D9djK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.2 U-Net\n","\n","Follow the code example for FCN and complete the following exercise."],"metadata":{"id":"LuMDaO2b9eki"}},{"cell_type":"markdown","source":["### (a) Data"],"metadata":{"id":"tFEeBraL9iD8"}},{"cell_type":"markdown","source":[" #### Download and visualization"],"metadata":{"id":"UgUzZ3dy-SzT"}},{"cell_type":"code","source":["# Download a brain image\n","! wget https://github.com/mateuszbuda/brain-segmentation-pytorch/raw/master/assets/TCGA_CS_4944.png -O  brain.png\n","\n","from PIL import Image\n","img = Image.open('brain.png')\n","\n","plt.rcParams[\"figure.figsize\"] = (5,5)\n","plt.imshow(img); plt.show()\n"],"metadata":{"id":"lQ7qNfNL9jBX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Pre-process [3 pts]"],"metadata":{"id":"Z98CEVUk9kSJ"}},{"cell_type":"code","source":["import numpy as np\n","from torchvision import transforms\n","\n","\n","# convert the image range to 0-1\n","img_arr = np.array(img)/255.\n","\n","\n","#### Your code starts here\n","# compute channel-wise mean and std\n","# need to feed it to: transforms.Normalize\n","noramlization_mean = ???\n","noramlization_std = ???\n","#### Your code ends here\n","\n","\n","preprocess = transforms.Compose([\n","    #### Your code starts here\n","    # apply channel-wise mean and std\n","\n","    #### Your code ends here\n","])\n","\n","# pre-process the image and make it 4-dimensional\n","input_batch = preprocess(img_arr).unsqueeze(0)"],"metadata":{"id":"LagK92k99lYI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (b) Model examination [2 pts]\n","\n","Write down the names of the top-level modules. Hints: it's an encoder-decoder architecture."],"metadata":{"id":"jt8ia-4m9mzR"}},{"cell_type":"code","source":["import torch\n","unet = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n","    in_channels=3, out_channels=1, pretrained=True)"],"metadata":{"id":"5SOSEBQL9oda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### Your code starts here\n","unet\n","\n","\"\"\"\n","-\n","-\n","-\n","\"\"\"\n","#### Your code ends here"],"metadata":{"id":"rNDeNozn-h_g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (c) Inference [2 pts]\n","\n","Hint: look at (3) in section 1.1\n"],"metadata":{"id":"MNkJgJAt9svm"}},{"cell_type":"code","source":["#### Your code starts here\n","\n","#### Your code ends here"],"metadata":{"id":"FL9DPf9Q9t3s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####  Visualization [2 pts]"],"metadata":{"id":"lZJWspwN9u8e"}},{"cell_type":"code","source":["plt.rcParams[\"figure.figsize\"] = (30,10)\n","\n","# image\n","plt.subplot(131)\n","plt.imshow(img);\n","plt.axis('equal');plt.axis('off')\n","\n","# binary segmentation. just display the seg and okay not to use the palette\n","plt.subplot(132)\n","#### Your code starts here\n","plt.imshow(???)\n","#### Your code ends here\n","plt.axis('equal');plt.axis('off')\n","\n","\n","# overlay segmentation onto the image\n","plt.subplot(133)\n","#### Your code ends here\n","plt.imshow(???)\n","#### Your code ends here\n","plt.axis('equal');plt.axis('off')\n","\n","plt.show()\n"],"metadata":{"id":"D_dzVGaX9v-F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-S5FO4QP1j9S"},"source":["# <b>2. Image generation</b>\n"]},{"cell_type":"markdown","source":["## 2.1 GAN (DCGAN)\n","\n","**Acknowledgement**: `Nathan Inkawhich <https://github.com/inkawhich>`"],"metadata":{"id":"lvoCnPMX610a"}},{"cell_type":"code","metadata":{"id":"nGJP6JKb2ik6"},"source":["from __future__ import print_function\n","#%matplotlib inline\n","import argparse\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","\n","# Set random seem for reproducibility\n","manualSeed = 999\n","#manualSeed = random.randint(1, 10000) # use if you want new results\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (a) Data"],"metadata":{"id":"s0cY5fJqaoWF"}},{"cell_type":"markdown","metadata":{"id":"txVm7Ay22uFa"},"source":["#### Download and pre-process"]},{"cell_type":"code","source":["! wget --no-check-certificate https://cseweb.ucsd.edu/~weijian/static/datasets/celeba/img_align_celeba.zip -O img_align_celeba.zip"],"metadata":{"id":"464Nu8IeZfF_"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGGjVTMQ2uRx"},"source":["import os\n","import zipfile\n","import gdown\n","import torch\n","from natsort import natsorted\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","\n","## Setup\n","# Number of gpus available\n","ngpu = 1\n","device = torch.device('cuda:0' if (\n","    torch.cuda.is_available() and ngpu > 0) else 'cpu')\n","\n","## Fetch data from Google Drive\n","# Root directory for the dataset\n","data_root = './'\n","# Path to folder with the dataset\n","dataset_folder = f'{data_root}/img_align_celeba'\n","# URL for the CelebA dataset\n","#url = 'https://drive.google.com/file/d/1qzaAz5CBvc9yUo1oLcauuY5tya1SArt2/'\n","# Path to download the dataset to\n","download_path = f'{data_root}/img_align_celeba.zip'\n","\n","# Create required directories\n","if not os.path.exists(data_root):\n","  os.makedirs(data_root)\n","  os.makedirs(dataset_folder)\n","\n","# Download the dataset from google drive\n","# gdown.download(url, download_path, quiet=False)\n","\n","# Unzip the downloaded file\n","with zipfile.ZipFile(download_path, 'r') as ziphandler:\n","  ziphandler.extractall(dataset_folder)\n","\n","## Create a custom Dataset class\n","class CelebADataset(Dataset):\n","  def __init__(self, root_dir, transform=None):\n","    \"\"\"\n","    Args:\n","      root_dir (string): Directory with all the images\n","      transform (callable, optional): transform to be applied to each image sample\n","    \"\"\"\n","    # Read names of images in the root directory\n","    image_names = os.listdir(root_dir)\n","\n","    self.root_dir = root_dir\n","    self.transform = transform\n","    self.image_names = natsorted(image_names)\n","\n","  def __len__(self):\n","    return len(self.image_names)\n","\n","  def __getitem__(self, idx):\n","    # Get the path to the image\n","    img_path = os.path.join(self.root_dir, self.image_names[idx])\n","    # Load image and convert it to RGB\n","    img = Image.open(img_path).convert('RGB')\n","    # Apply transformations to the image\n","    if self.transform:\n","      img = self.transform(img)\n","\n","    return img\n","\n","## Load the dataset\n","# Path to directory with all the images\n","img_folder = f'{dataset_folder}/img_align_celeba'\n","# Spatial size of training images, images are resized to this size.\n","image_size = 64\n","# Transformations to be applied to each individual image sample\n","transform=transforms.Compose([\n","    transforms.Resize(image_size),\n","    transforms.CenterCrop(image_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                          std=[0.5, 0.5, 0.5])\n","])\n","# Load the dataset from file and apply transformations\n","celeba_dataset = CelebADataset(img_folder, transform)\n","\n","## Create a dataloader\n","# Batch size during training\n","batch_size = 128\n","# Number of workers for the dataloader\n","num_workers = 0 if device.type == 'cuda' else 1\n","# Whether to put fetched data tensors to pinned memory\n","pin_memory = True if device.type == 'cuda' else False\n","\n","celeba_dataloader = torch.utils.data.DataLoader(celeba_dataset,\n","                                                batch_size=batch_size,\n","                                                num_workers=num_workers,\n","                                                pin_memory=pin_memory,\n","                                                shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xa3v0p-fK0SQ"},"source":["#### Visualization"]},{"cell_type":"code","metadata":{"id":"RACTGq7p4qjg"},"source":["# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","\n","# Plot some training images\n","real_batch = next(iter(celeba_dataloader))\n","plt.figure(figsize=(10,10))\n","plt.axis(\"off\")\n","plt.title(\"Training Images\")\n","plt.imshow(np.transpose(vutils.make_grid(real_batch.to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wUn5N_ZJ2mCZ"},"source":["### (b) Model\n","\n"]},{"cell_type":"markdown","source":["#### Model definition"],"metadata":{"id":"tljbUTrG4VCI"}},{"cell_type":"code","metadata":{"id":"p2sg1W6E5a_r"},"source":["# Generator Code\n","\n","class Generator(nn.Module):\n","    def __init__(self, ngpu):\n","        super(Generator, self).__init__()\n","        self.ngpu = ngpu\n","        self.main = nn.Sequential(\n","            # input is Z, going into a convolution\n","            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","            # state size. (ngf*8) x 4 x 4\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","            # state size. (ngf*4) x 8 x 8\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","            # state size. (ngf*2) x 16 x 16\n","            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","            # state size. (ngf) x 32 x 32\n","            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","            # state size. (nc) x 64 x 64\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EjntWz25YNF"},"source":["class Discriminator(nn.Module):\n","    def __init__(self, ngpu):\n","        super(Discriminator, self).__init__()\n","        self.ngpu = ngpu\n","        self.main = nn.Sequential(\n","            # input is (nc) x 64 x 64\n","            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf) x 32 x 32\n","            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*2) x 16 x 16\n","            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*4) x 8 x 8\n","            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*8) x 4 x 4\n","            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IaVLFe17LBvk"},"source":["#### Model Initialization"]},{"cell_type":"code","metadata":{"id":"pPwgJyly2mLM"},"source":["# Batch size during training\n","batch_size = 128\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 64\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","# Size of feature maps in generator\n","ngf = 64\n","# Size of feature maps in discriminator\n","ndf = 64\n","# Number of training epochs\n","num_epochs = 5\n","# Learning rate for optimizers\n","lr = 0.0002\n","# Beta1 hyperparam for Adam optimizers\n","beta1 = 0.5\n","# Number of GPUs available. Use 0 for CPU mode.\n","ngpu = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUfunGki5gcf"},"source":["# Create the generator\n","netG = Generator(ngpu).to(device)\n","netD = Discriminator(ngpu).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHE_NNwwLPrT"},"source":["### (c) Loss function"]},{"cell_type":"code","metadata":{"id":"NCyqkF4g5u71"},"source":["# Initialize BCELoss function\n","criterion = nn.BCELoss()\n","\n","# Create batch of latent vectors that we will use to visualize\n","#  the progression of the generator\n","fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n","\n","# Establish convention for real and fake labels during training\n","real_label = 1\n","fake_label = 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wfyq0St0LSzb"},"source":["### (d) Optimization"]},{"cell_type":"code","metadata":{"id":"0oMX32vv6T8t"},"source":["# Setup Adam optimizers for both G and D\n","optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n","optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VENW03MPLX9h"},"source":["### (e) Start Training"]},{"cell_type":"code","metadata":{"id":"f0nkU94G5xXD"},"source":["# Training Loop\n","\n","# Lists to keep track of progress\n","img_list = []\n","G_losses = []\n","D_losses = []\n","iters = 0\n","\n","plt.figure(figsize=(10,10))\n","print(\"Starting Training Loop...\")\n","# For each epoch\n","for epoch in range(num_epochs):\n","    # For each batch in the dataloader\n","    for i, data in enumerate(celeba_dataloader, 0):\n","\n","        ############################\n","        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","        ###########################\n","        ## 1.1 Train with all-real batch\n","        netD.zero_grad()\n","        # Format batch\n","        real_cpu = data.to(device)\n","        b_size = real_cpu.size(0)\n","        label = torch.full((b_size,), 0, device=device).type(torch.float32)\n","        # Forward pass real batch through D\n","        output = netD(real_cpu).view(-1)\n","        # Calculate loss on all-real batch\n","        label.fill_(real_label)\n","        errD_real = criterion(output, label)\n","        # Calculate gradients for D in backward pass\n","        errD_real.backward()\n","        D_x = output.mean().item()\n","\n","        ## 1.2 Train with all-fake batch\n","        # Generate batch of latent vectors\n","        noise = torch.randn(b_size, nz, 1, 1, device=device)\n","        # Generate fake image batch with G\n","        fake = netG(noise)\n","        # Classify all fake batch with D\n","        output = netD(fake.detach()).view(-1)\n","        # Calculate D's loss on the all-fake batch\n","        label.fill_(fake_label)\n","        errD_fake = criterion(output, label)\n","        # Calculate the gradients for this batch\n","        errD_fake.backward()\n","        D_G_z1 = output.mean().item()\n","        # Add the gradients from the all-real and all-fake batches\n","        errD = errD_real + errD_fake\n","        # Update D\n","        optimizerD.step()\n","\n","        ############################\n","        # (2) Update G network: maximize log(D(G(z)))\n","        ###########################\n","        netG.zero_grad()\n","        label.fill_(real_label)  # fake labels are real for generator cost\n","        # Since we just updated D, perform another forward pass of all-fake batch through D\n","        output = netD(fake).view(-1)\n","        # Calculate G's loss based on this output\n","        errG = criterion(output, label)\n","        # Calculate gradients for G\n","        errG.backward()\n","        D_G_z2 = output.mean().item()\n","        # Update G\n","        optimizerG.step()\n","\n","        # Output training stats\n","        if i % 50 == 0:\n","            dsp_str = '[%d/%d][%d/%d]' % (epoch, num_epochs, i, len(celeba_dataloader))\n","            print('%s\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n","                  % (dsp_str, errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n","            # Check how the generator is doing by saving G's output on fixed_noise\n","            with torch.no_grad():\n","                fake = netG(fixed_noise).detach().cpu()\n","            fake = vutils.make_grid(fake[:64], padding=2, normalize=True)\n","            plt.axis(\"off\")\n","            plt.title(\"Generated fake Images\")\n","            plt.imshow(np.transpose(vutils.make_grid(fake, padding=2, normalize=True),(1,2,0)))\n","            plt.pause(1)\n","\n","\n","        # Save Losses for plotting later\n","        G_losses.append(errG.item())\n","        D_losses.append(errD.item())\n","\n","        iters += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 Text-based image generation\n","Latest technology!\n","\n","Acknowledgement: https://huggingface.co/CompVis/stable-diffusion-v1-4"],"metadata":{"id":"fGo9sSIH7oAt"}},{"cell_type":"markdown","source":["### (a) Setup"],"metadata":{"id":"qMRvVE3B_MrQ"}},{"cell_type":"code","source":["! pip install --upgrade diffusers transformers scipy"],"metadata":{"id":"Pp8Qr0vP7tf_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from diffusers import StableDiffusionPipeline\n","import matplotlib.pyplot as plt\n","\n","model_id = \"CompVis/stable-diffusion-v1-4\"\n","device = \"cuda\"\n","pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n","pipe = pipe.to(device)"],"metadata":{"id":"g5HUjDkz7-py"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (b) Inference"],"metadata":{"id":"OYP6FAXI_SPK"}},{"cell_type":"code","source":["prompt = \"a photo of an astronaut riding a horse on mars\"\n","image = pipe(prompt).images[0]\n","plt.imshow(image)\n","plt.show()"],"metadata":{"id":"PntQKoyf8J-z"},"execution_count":null,"outputs":[]}]}