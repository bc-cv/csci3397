{"cells":[{"cell_type":"markdown","metadata":{"id":"5Ag0r1CeJ-7X"},"source":["# CSCI 3397 Lab 9: Object Detection\n","\n","**Posted:** Friday, April 12, 2024\n","\n","**Due:** Wednesday, April 17, 2024\n","\n","__Total Points__: 12 pts\n","\n","__Submission__: please rename the .ipynb file as __\\<your_username\\>\\_lab9.ipynb__ before you submit it to canvas. Example: weidf_lab9.ipynb."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKO-UCCkOPGG"},"outputs":[],"source":["# Download images: same as in Lab 9a\n","! wget https://bc-cv.github.io/csci3397/public/object_detection/test_dog_easy.jpg -O test_dog_easy.jpg\n","! wget https://bc-cv.github.io/csci3397/public/object_detection/test_dog_hard.jpg -O test_dog_hard.jpg\n","\n","# Download ImageNet labels\n","! wget https://bc-cv.github.io/csci3397/public/object_detection/imagenet_classes.txt -O imagenet_classes.txt\n","with open(\"imagenet_classes.txt\", \"r\") as f:\n","    categories = [s.strip() for s in f.readlines()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJ4_ORp6OVKj"},"outputs":[],"source":["import imageio\n","import matplotlib.pyplot as plt\n","\n","dog_easy = imageio.imread('test_dog_easy.jpg')\n","dog_hard = imageio.imread('test_dog_hard.jpg')\n","\n","plt.rcParams[\"figure.figsize\"] = (10,5)\n","plt.subplot(121)\n","plt.imshow(dog_easy)\n","plt.axis('off')\n","plt.title('easy case')\n","plt.subplot(122)\n","plt.imshow(dog_hard)\n","plt.axis('off')\n","plt.title('hard case')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HyVwnm9R9iH4"},"source":["Let's try out different pipelines for the hard case above to detect the dog."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3gwvt-BocmE"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import numpy as np\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","# Download models\n","alexnet = torchvision.models.alexnet(pretrained=True)"]},{"cell_type":"markdown","metadata":{"id":"ZJMh85oLauMX"},"source":["# <b> 1. Sliding CNN</b>\n","Let's iterative through the bounding boxes with a certain size (square patch: 151x151) with a fixed stride (71x71). In practice, we need to repeat the computation above for different sizes of bounding boxes."]},{"cell_type":"markdown","metadata":{"id":"s2pzsuX1vb1A"},"source":["## (i) Get bounding boxes (sliding window)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8P-YZw3bTIl"},"outputs":[],"source":["def imageToPatch(img, row_id, col_id, patch_size, stride_size):\n","  return img[stride_size*row_id : stride_size*row_id+patch_size,\\\n","             stride_size*col_id : stride_size*col_id+patch_size]\n","\n","# image to patches\n","dog_hard = imageio.imread('test_dog_hard.jpg')\n","im_size = dog_hard.shape\n","patch_size = 151\n","stride_size = 71\n","num_row = (im_size[0] - patch_size) // stride_size + 1\n","num_col = (im_size[1] - patch_size) // stride_size + 1\n","print('#row=%d, #col=%d' % (num_row, num_col))\n","\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","count = 1\n","for y in range(num_row):\n","  for x in range(num_col):\n","    plt.subplot(num_row, num_col, count)\n","    patch = imageToPatch(dog_hard, y, x, patch_size, stride_size)\n","    plt.imshow(patch)\n","    plt.axis('off')\n","    count += 1\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WC_V4G0KvjE_"},"source":["## (ii) Preprocess image patches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9eLGG_0v9iO-"},"outputs":[],"source":["preprocess = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# convert image into patches\n","patches = []\n","for y in range(num_row):\n","  for x in range(num_col):\n","    patch = imageToPatch(dog_hard, y, x, patch_size, stride_size)\n","    patches.append(preprocess(Image.fromarray(patch)))\n","\n","im_batch = torch.stack(patches)\n","print('input batch size:', im_batch.shape)"]},{"cell_type":"markdown","metadata":{"id":"GyOTqTpkvxm4"},"source":["## (iii) Run inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgLww1S9bMPT"},"outputs":[],"source":["pred = alexnet(im_batch)\n","prob = F.softmax(pred, dim=1).detach().numpy()\n","prob_max = prob.max(axis=1)\n","\n","\n","# plot the top probability for each patch\n","plt.rcParams[\"figure.figsize\"] = (10,2)\n","plt.plot(prob_max)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NkkqAn91duDB"},"outputs":[],"source":["# let's see what does it detect for the most confident patch\n","pos_ids = np.where(prob_max > 0.3)[0]\n","row_ids = pos_ids // num_col\n","col_ids = pos_ids - row_ids * num_col\n","\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","for i in range(len(pos_ids)):\n","  plt.subplot(3,3,i+1)\n","  patch = imageToPatch(dog_hard, row_ids[i], col_ids[i], patch_size, stride_size)\n","  plt.imshow(patch)\n","  plt.axis('off')\n","  plt.title(categories[np.argmax(prob[pos_ids[i]])])"]},{"cell_type":"markdown","metadata":{"id":"vPU2lxdFAIay"},"source":["# <b>2. R-CNN</b>\n"]},{"cell_type":"markdown","metadata":{"id":"7rwU_9uFvY0a"},"source":["## (i) Get bounding boxes (selective search)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLHgnXlZAIjC"},"outputs":[],"source":["# cv2 reads in images as BGR; pytorch pretrain models take RGB input\n","# create a new variable to avoid confusion...\n","dog_hard_cv2 = cv2.imread('test_dog_hard.jpg')\n","ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n","ss.setBaseImage(dog_hard_cv2)\n","ss.switchToSelectiveSearchFast()\n","# rects: Nx4 matrix\n","# each row: x,y,w,h\n","rects = ss.process()\n","print('Selective search: find %d boxes' % rects.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9quSU8JjhMP2"},"outputs":[],"source":["# plot the top 6 bounding boxes\n","image = cv2.rectangle(dog_hard_cv2.copy(), tuple(rects[0,:2]), tuple(rects[0,:2]+rects[0,2:]), (0,255,0), 2)\n","for i in range(1, 6):\n","  image = cv2.rectangle(image, tuple(rects[i,:2]), tuple(rects[i,:2]+rects[i,2:]), (0,255,0), 2)\n","cv2_imshow(image)"]},{"cell_type":"markdown","metadata":{"id":"x5KjuWLij7IH"},"source":["## (ii) Preprocess image patches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaZFtxAGh_4M"},"outputs":[],"source":["preprocess = transforms.Compose([\n","    transforms.Resize((224,224)), # scale the both sides to 224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# take every 8 bounding boxes\n","rects_sub = rects[::8]\n","patches = [None] * rects_sub.shape[0]\n","for i in range(len(patches)):\n","  x,y,w,h = rects_sub[i]\n","  patches[i] = preprocess(Image.fromarray(dog_hard[y:y+h, x:x+w]))\n","\n","im_batch = torch.stack(patches)\n","print('input batch size:', im_batch.shape)"]},{"cell_type":"markdown","metadata":{"id":"39o3d6wEvWPK"},"source":["## (iii) [4 pts] Exercise 1: Run inference\n","Repeat the code in `(1,i) Sliding CNN` to plot the top-1 prediction for all the patches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VKQIz9ClnMr"},"outputs":[],"source":["#### Your code starts here ####\n","# compute the top-class probability for each patch\n","\n","# plot the top probability for each patch\n","\n","#### Your code ends here ####\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZ8b5fNxks_9"},"outputs":[],"source":["# let's see what does it detect for the most confident patch\n","pos_sort = np.argsort(-prob_max)\n","\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","for i in range(16):\n","  plt.subplot(4,4,i+1)\n","  x,y,w,h = rects_sub[pos_sort[i]]\n","  patch = dog_hard[y:y+h, x:x+w]\n","  plt.imshow(cv2.resize(patch,(224,224)))\n","  plt.axis('off')\n","  plt.title(categories[np.argmax(prob[pos_sort[i]])])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRKk5QpaqepN"},"outputs":[],"source":["# Non-maximum suppression\n","# https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\n","rects_sub_pt = torch.as_tensor(np.hstack([rects_sub[:,:2], rects_sub[:,:2]+rects_sub[:,2:]]).astype(np.float32))\n","idx = torchvision.ops.nms(rects_sub_pt, torch.as_tensor(prob_max), 0.1)\n","\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","for i in range(16):\n","  plt.subplot(4,4,i+1)\n","  x,y,w,h = rects_sub[idx[i]]\n","  patch = dog_hard[y:y+h, x:x+w]\n","  plt.imshow(cv2.resize(patch,(224,224)))\n","  plt.axis('off')\n","  plt.title(categories[np.argmax(prob[idx[i]])])"]},{"cell_type":"markdown","metadata":{"id":"pGtKW3oHxRA-"},"source":["# <b> 3. Fast/Faster R-CNN (Detectron2!)</b>"]},{"cell_type":"markdown","metadata":{"id":"oFWLXXtZy6J3"},"source":["## (i) Install Detectron2\n","\n","Remember to restart the runtime after the following installation. Click the \"restart runtime\" button in the code block output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlui9fE9sQfO"},"outputs":[],"source":["# !pip install pyyaml==5.3.1\n","# This is the current pytorch version on Colab. Uncomment this if Colab changes its pytorch version\n","!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","# Install detectron2 that matches the above pytorch version\n","# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.9/index.html\n","\n","exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"]},{"cell_type":"markdown","metadata":{"id":"sML_VsOk3I7G"},"source":["## (ii) Run inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1FfQCr66qIeO"},"outputs":[],"source":["# check pytorch installation:\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","assert torch.__version__.startswith(\"1.9\")   # please manually install torch 1.9 if Colab changes its default version\n","\n","# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"izX0S6TIy4rH"},"outputs":[],"source":["cfg = get_cfg()\n","cfg.INPUT.FORMAT = 'RGB'\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_C4_3x.yaml\"))\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_C4_3x.yaml\")\n","predictor = DefaultPredictor(cfg)\n","\n","# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n","\n","# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n","\n","#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPcLLtaZ0Dvm"},"outputs":[],"source":["import imageio\n","dog_hard = imageio.imread('test_dog_hard.jpg')\n","outputs = predictor(dog_hard)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94JyU9-y1eCX"},"outputs":[],"source":["# examine what's in the output: {bounding box, class probability, detection confidence}\n","outputs['instances'].__dict__['_fields'].keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UpbnNvP110aX"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# We can use `Visualizer` to draw the predictions on the image.\n","v = Visualizer(dog_hard, MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n","out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","\n","plt.rcParams[\"figure.figsize\"] = (30,30)\n","plt.imshow(out.get_image())\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","source":["## (iii) Model examination\n","Given a pytorch model, we can examine the following two aspects:\n","- Layer: how the lego pieces are organized into blocks\n","- Feature maps: output size of specific layers"],"metadata":{"id":"m8_RGP7kIzRO"}},{"cell_type":"markdown","metadata":{"id":"3EAy2ZBT3SSn"},"source":["### (a) Layer examination\n","\n","As we've learned, one neural network can have hundreds of layers. Thus, the layers are further grouped into computation blocks for a better organization."]},{"cell_type":"code","source":["# print all module names (look for the top-level hierarchy)\n","predictor.model._modules"],"metadata":{"id":"DrKenbh4T0CG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictor.model._modules.keys()"],"metadata":{"id":"wg5aYUSXKKMg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Thus, here are the name of modules on the top-level hierarchy in the model:\n","```\n","Faster-RCNN:\n","- backbone\n","- proposal_generator\n","- roi_heads\n","```\n","\n","As learned in class, given the image feature map from `backbone`, `prorposal_generator` predicts possible bounding boxes and `roi_reads` crops+resizes the feature map for each bounding box and predict the class probability for the object inside the box."],"metadata":{"id":"lmHDZd1sKLfY"}},{"cell_type":"markdown","metadata":{"id":"NpSGDYzZ4EK0"},"source":["### (b) Feature map examination\n","\n","To save memory, pytorch doesn't save the intermediate layer output (feature maps) by default. We can apply the `register_forward_hook()` function to the layer for examination.\n","\n","For example, to check the size of the feature map of the **backbone**, we can look into its last computation block (second-level layer blocks) `res4` as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gb_0Rno56yZW"},"outputs":[],"source":["## code to show modules for the Detectron2 model\n","activation = {}\n","def get_activation(name):\n","    def hook(model, input, output):\n","        activation[name] = output.detach()\n","    return hook\n","\n","predictor.model._modules['backbone']._modules['res4'].register_forward_hook(get_activation('res4'))\n","outputs = predictor(dog_hard)\n","\n","print('res4-block output feature size: ', activation['res4'].shape)"]},{"cell_type":"markdown","metadata":{"id":"LhLE2aBjEPP0"},"source":["# <b> 4. YOLO </b>"]},{"cell_type":"markdown","metadata":{"id":"6UaHCaMgEYCa"},"source":["## (i) Install YOLO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yu1JhiXjEYLj"},"outputs":[],"source":["! pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies"]},{"cell_type":"markdown","metadata":{"id":"FkJZn3jVEpVv"},"source":["## (ii) Run inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2MB4toHEper"},"outputs":[],"source":["import torch\n","\n","# Model\n","model_yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n","\n","# Images\n","imgs = ['https://bc-cv.github.io/csci3397/public/object_detection/test_dog_hard.jpg']  # batch of images\n","\n","# Inference\n","results = model_yolo(imgs)\n","results_np = results.xyxy[0].detach().cpu().numpy().astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EGQkpDOHLLu"},"outputs":[],"source":["# plot the top 6 bounding boxes\n","image = cv2.rectangle(cv2.imread('test_dog_hard.jpg'), tuple(results_np[0,:2]), tuple(results_np[0,2:4]), (0,255,0), 2)\n","for i in range(1, results_np.shape[0]):\n","  image = cv2.rectangle(image, tuple(results_np[i,:2]), tuple(results_np[i,2:4]), (0,255,0), 2)\n","cv2_imshow(image)"]},{"cell_type":"code","source":["%matplotlib inline\n","plt.rcParams[\"figure.figsize\"] = (20,20)\n","for i in range(8):\n","  plt.subplot(4,2,i+1)\n","  x1,y1,x2,y2,sc, cls = results_np[i]\n","  patch = dog_hard[y1:y2, x1:x2]\n","  # cv2_imshow uses BGR instead of RGB...\n","  # cv2_imshow(cv2.resize(patch,(224,224))[:,:,::-1])\n","  plt.imshow(cv2.resize(patch,(224,224)))\n","  plt.axis('off')\n","  plt.title(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes[int(cls)])\n","plt.show()"],"metadata":{"id":"wqLY2GE4NKLF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1ofk2ZzIBt6"},"source":["## [4 pts] Exercise 2.1: Layer examination\n","List the name of modules on the top-level hierarchy in the model."]},{"cell_type":"code","source":["model_yolo._modules['model']._modules['model']._modules['model']._modules"],"metadata":{"id":"dX6I1O2tM43e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["YOLO\n","```\n","# Your output starts here\n","-\n","-\n","-\n","# Your output ends here\n","```\n"],"metadata":{"id":"w44AwPX0XI-9"}},{"cell_type":"markdown","source":["## [4 pts] Exercise 2.2: Data examination\n","\n","Print the size of the output of the `24`-th module.\n","\n","Hint:\n","- use `._modules.keys()` to make sure you are registrating the hook to the module_list instead of a tuple/dictionary.\n","- if you make a mistake for the hook registeration, you may need to reload the model to start over.\n","- note that as the `__forward__()` function has more processing, the output from `model_yolo(im)` is different from the raw output of the last computational block (module list)."],"metadata":{"id":"uGqpfzLcMz3S"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2I7KlKQIB2L"},"outputs":[],"source":["# Your code starts here\n","\n","# Your code ends here\n","print('yolo detect output feature size: ', activation['yolo_out'].shape)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}